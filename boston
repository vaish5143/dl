# Importing necessary libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Ensure that the "boston_housing.csv" is in the correct directory
data = pd.read_csv("boston_housing.csv")

# Drop any rows with missing values
data.dropna(inplace=True)

# Prepare the feature matrix (X) and target vector (y)
X = data.drop("MEDV", axis=1)  # Features: All columns except the target column 'MEDV'
y = data["MEDV"]  # Target: 'MEDV' is the median house price

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the features using StandardScaler for faster neural network convergence
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build the deep neural network model
model = tf.keras.Sequential([
    tf.keras.Input(shape=(X_train.shape[1],)),  # Input layer with the number of features
    tf.keras.layers.Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation
    tf.keras.layers.Dropout(0.2),  # Dropout for regularization
    tf.keras.layers.Dense(64, activation='relu'),  # Second hidden layer with 64 neurons and ReLU activation
    tf.keras.layers.Dropout(0.2),  # Dropout for regularization
    tf.keras.layers.Dense(1)  # Output layer with 1 neuron (since it's a regression problem)
])

# Compile the model with Adam optimizer and Mean Squared Error (MSE) loss function
model.compile(optimizer='adam', loss='mse')

# Train the model for 100 epochs
model.fit(X_train, y_train, epochs=100, verbose=0)

# Make predictions on the test data
y_pred = model.predict(X_test).flatten()  # Flattening the predictions for compatibility

# Evaluate the model using various regression metrics
mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error
mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error
rmse = np.sqrt(mse)  # Root Mean Squared Error
r2 = r2_score(y_test, y_pred)  # R-squared (coefficient of determination)

# Print the evaluation metrics
print("MAE: ", mae)
print("MSE: ", mse)
print("RMSE: ", rmse)
print("R2 Score: ", r2)

# Visualization: Actual vs Predicted Values
plt.scatter(x=y_test, y=y_pred)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Ideal line (y=x)
plt.show()

# Optional: Seaborn Regression Plot
sns.regplot(x=y_test, y=y_pred)
plt.title("Regression Line for Predicted Values")
plt.show()
